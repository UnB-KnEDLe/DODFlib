# -*- coding: utf-8 -*-
"""cnn_cnn_lstm__dodflib.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qx2kg6p4NB-KEKhgFxIf6soeL0dZqgbr
"""

from google.colab import drive
# from pathlib import Path
drive.mount('/content/drive')

!pip install --upgrade gensim

!pip install seqeval

# Basic packages
import itertools
import joblib
import re
import math
import operator
import argparse
import torch
import numpy as np
import pandas as pd

from pathlib import Path
from torch.utils.data import DataLoader
# NER open packages
from seqeval.scheme import IOBES
from seqeval.metrics import f1_score
# my NER packages
# from data3 import active_dataset
import metrics
import data3
import utils
from CNN_biLSTM_CRF import cnn_bilstm_crf
from CNN_CNN_LSTM2 import CNN_CNN_LSTM

def devicefy(lis, device):
  return [i.to(device) for i in lis]

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")


DATA_PATH=Path('drive/MyDrive/knedle_data/')
SAVE_PATH=Path('drive/MyDrive/knedle_data/NER_results')


from importlib import reload as rl
import utils
rl(utils)
rl(data3)


def pre_model_builder(dataset, emb_path):
  global DATA_PATH, batch_size, lstm_hidden_size
  global emb, train_path, test_path, data_format
  batch_size = 16
  data_format = 'iob2'
  lstm_hidden_size = 128
  train_path = DATA_PATH / f'{dataset}/{dataset}_train.txt'
  test_path = DATA_PATH / f'{dataset}/{dataset}_test.txt'
  emb_path = Path('drive/MyDrive/knedle_data/foo.kv')
  emb = utils.load_embedding(
      dataset, 
      train_path, 
      test_path, 
      embedding_path=emb_path,
      embedding_size=50, 
      data_format='iob2',
    )

  global collate_object
  collate_object = utils.new_custom_collate_fn(
      pad_idx=emb.key_to_index['<PAD>'], 
      unk_idx=emb.key_to_index['<UNK>'],
  )

  print('\nGenerating text2idx dictionaries (word, char, tag)')
  global word2idx, char2idx, tag2idx

  word2idx = utils.create_word2idx_dict(
      emb.index_to_key, 
      emb.key_to_index, 
      train_path)
  char2idx = utils.create_char2idx_dict(train_path=train_path)
  tag2idx  = utils.create_tag2idx_dict(train_path=train_path)

  params = dict(
    batch_size=16,
    data_format='iob2',
    lstm_hidden_size=128,
    train_path=DATA_PATH / f'{dataset}/{dataset}_train.txt',
    test_path=DATA_PATH / f'{dataset}/{dataset}_test.txt',
    emb_path=Path('drive/MyDrive/knedle_data/foo.kv'),
    emb=emb,
    collate_object=collate_object,
    word2idx=word2idx,
    char2idx=char2idx,
    tag2idx=tag2idx,
  )
  return params


def model_builder(model_opt):

  global lstm_hidden_size
  global char2idx, emb, word2idx, tag2idx, DEVICE
  global lrate, momentum, clipping_value
  global model

  if model_opt == 'CNN-CNN-LSTM':
      model = CNN_CNN_LSTM(char_vocab_size=len(char2idx),
            char_embedding_dim=25,
            char_out_channels=50,
            pretrained_word_emb=emb,
            num_classes=len(tag2idx),
            device=DEVICE
            
            word2idx = word2idx,
            word_out_channels=400,
            word_conv_layers = 1,
            decoder_layers = 1,
            decoder_hidden_size = 128,
      )      
  elif model_opt == 'CNN-biLSTM-CRF':
      model = cnn_bilstm_crf(char_vocab_size=len(char2idx), 
                    char_embedding_dim=30, 
                    char_out_channels=30, 
                    pretrained_word_emb=emb, 
                    num_classes=len(tag2idx), 
                    device=DEVICE, 
                    lstm_hidden_size=lstm_hidden_size,
      )
  lrate = 0.0015
  momentum = 0.9
  clipping_value = 5.0

  model = model.to(DEVICE)


def datasets_builder():
  print('\nCreating training dataset')
  global train_path, word2idx, char2idx, tag2idx, data_format
  global train_set
  global batch_size

  train_set = data3.active_dataset(
      data=train_path, 
      word2idx_dic=word2idx, 
      char2idx_dic=char2idx, 
      tag2idx_dic=tag2idx, 
      data_format=data_format)
  # Putting all sentences into the labeled set for training
  train_set.flag_labeled = False
  train_set.label_data( [*range(len(train_set))] )
  train_set.flag_labeled = True



  train_set.flag_labeled = True
  batch_size = 16
  global train_dataloader
  train_dataloader = DataLoader(
      train_set, 
      batch_size=batch_size, 
      pin_memory=True, collate_fn = collate_object, shuffle=False)

  print('\nCreating test dataset')
  global test_set
  test_set  = data3.active_dataset(
      data=test_path, 
      word2idx_dic=word2idx, 
      char2idx_dic=char2idx, 
      tag2idx_dic=tag2idx, 
      data_format=data_format)
  # Putting all sentences into the labeled set for testing
  test_set.flag_labeled = False
  test_set.label_data([*range(len(test_set))])
  test_set.flag_labeled = True
  global test_dataloader
  test_dataloader = DataLoader(test_set, batch_size=128, shuffle=False, collate_fn=collate_object)


def post_model_builder():
  global parser_opt, model, lrate, momentum
  epochs = 20
  
  global optim
  optim = torch.optim.SGD(model.parameters(), lr=lrate, momentum=momentum)
  return optm, epochs

rl(metrics)
rl(utils)
rl(data3)
epochs = 10
ACTS = [
  # 'aposentadoria',
  'ato_nomeacao_efetivo',
  'ato_exoneracao_efetivo',
  'ato_cessao',
  'ato_exoneracao_comissionado',
  'ato_nomeacao_comissionado',
  'ato_retificacao_comissionado',
  'ato_retificacao_efetivo',
  'ato_reversao',
  'ato_substituicao',
  'ato_tornado_sem_efeito_apo',
  'ato_tornado_sem_efeito_exo_nom',
  'ato_abono_permanencia',
]
which_act = 2
skept = []
errors = []
for act in ACTS[:3]:

  pre_model_builder(act)
  model_builder('CNN-CNN-LSTM')
  if act == 'aposentadoria':
    batch_size = 64
  datasets_builder()

  if len(train_dataloader) < 6:
    print("skipping...", act)
    skept.append(act)
    continue
  post_model_builder()
  
  f1_history = []
  print('ACT:', act)
  for epoch in range(epochs if batch_size == 64 else 10):
    print(f'\tEpoch: {epoch}')
    model.train()        
    # for sent, tag, word, mask in dataloader:
    for tup in train_dataloader:
      sent, tag, word, mask = devicefy(tup, DEVICE)
      optim.zero_grad()
      loss = model(sent, word, tag, mask)
      loss.backward()
      torch.nn.utils.clip_grad_norm_(model.parameters(), clipping_value)
      optim.step()
    
    # Verify performance on test set after supervised training
    model.eval()
    with torch.no_grad():
      predictions, targets = metrics.preprocess_pred_targ(model, test_dataloader, DEVICE)
      predictions = metrics.IOBES_tags(predictions, tag2idx)
      targets = metrics.IOBES_tags(targets, tag2idx)
      micro_f1 = f1_score(targets, predictions, mode='strict', scheme=IOBES)
      f1_history.append(0 if np.isnan(micro_f1) else micro_f1)
      print(f'\tmicro f1-score: {micro_f1}\n')
  break


